---
title: "Project Title"
subtitle: "Author Names"
format: pdf
editor: visual
---

# Abstract

Salaries vary greatly across countries due to multiple factors, but the key factors that influence salaries are unclear. In this project, we aim to investigate predictors of salary, including education, experience, industry, gender, and race. Using over 28,000 survey responses from AskAManager.org and data cleaning, we focused on analyzing the US, UK, and Canada for our research question: What are the key factors that influence salaries within each country, and how do these factors differ across countries? We then applied a Random Forest Classifier model to look at salary factors and to evaluate which feature was the most important. Our results showed that industry, education level, and gender are the biggest predictors of salaries. Through our exploratory data analysis, we also saw that years of experience correlates with salary. Our project suggests that education and industry matter most and don't matter with countries when determining salary levels. 

# Introduction

Understanding key factors for high salaries is important for economic opportunities and not many studies focus on many factors; they often only focus on a single factor like gender or education. Research such as Blau and Kahn's Gender Wage Gap (2017) highlights this, but there is a gap in knowledge when it comes to multiple factors. Through this project, we aim to observe the importance of different factors and how they affect salaries. For example, is education or experience more important for a higher paying job? Salaries differ across countries, and we can explore these changes based on different factors.

We used survey data from AskAManager.org, which had over 28,000 responses globally. We narrowed our focus to the US, UK, and Canada, as they dominated the number of responses in the survey (around 26,000). This dataset gave us a good opportunity to look at different demographics and professions, and also to compare between countries. Additionally, it was a great dataset to practice real-world data cleaning as it was quite messy. Our approach for data cleaning focused on normalizing currencies, experience binning, and handling missing or extra values. In our exploratory data analysis, we visualized salary distributions across key variables. Ultimately, we chose to use a Random Forest classifier because it handles mixed data well, can capture non-linear patterns and interactions between variables, which can give us feature importance, helping us identify which variables most impact salary across countries (Breiman 2001). 

Our research provided us with new insights: being in the tech industry, having a master’s degree and being a woman are the most important salary predictors according to our model. Surprisingly, the location doesn’t seem to have a big impact on the predictions of salaries. Data analysis revealed different results: the most influential factors were having a PhD, 20-30 years of experience, being male, and being in the tech industry. 


# Methods

Our project's data comes from an online Google Forms survey that was open worldwide on AskAManager.org's website. The dataset has 17 variables; the quantitative variables were the base salary, bonus income, and years of experience. The categorical variables were country, industry, education level, gender, and race.  Six of the columns are free-response entries that were an addition to the main required questions, which we removed during our data cleaning. The dataset contains more than 27,000 individual data points. Around 26,000 data points were focused on the US, UK, and Canada. The project workflow is highlighted in Figure 1.
![Figure 1](https://drive.google.com/file/d/1Dr7B766tN8l9gZ00ZU_TbFoM-JFCzYuy/view?usp=sharing)

# Data Cleaning

Surveys are a great method of data collection however they often suffer from issues due to free response answers and incomplete answers. These unconstrained answers and null values complicate our ability to use this data to make predictions or identify patterns in the data and thus a level of data cleaning is required before performing any further analysis on this data. 

Our first steps were to ensure consistency throughout the data where we performed several preprocessing steps aimed at ensuring standardization of values to prepare for data analysis. This started with consolidating compensation data into a single value where we combined bonuses and base pay all into one new column named total compensation. Additionally, as compensation was reported in each respondents corresponding national currency, we created a new column which was the respondents total compensation normalized into USD using current exchange rates.

Next, we decided to remove extraneous columns and data automatically provided by google surveys that didn’t match the context of our analysis or provided hard to classify data. This culminated in the removal of the timestamp column, job context and job title columns. The timestamp column just provided the time the form was submitted which provided little to no discernable data in regards to the person submitting. The job context and the job title columns did provide useful information, however, the responses in these columns were often incredibly verbose and provided far too much detail in regards to the respondents job description to the point it was far easier to group professions based on the industry column rather than attempting to bucket job titles or job context responses into a feasible number of classifications. 

Because our dataset contained so many values, I decided it was best to just drop data points that were missing data in crucial categories such as gender, race, education level and industry. These categories were important for our analysis on demographic and occupational breakdowns. We considered using data augmentation techniques as an alternative to just dropping values however these attributes followed no structured pattern that would have allowed for common augmentation techniques. Given the size of the dataset, removing this sample of data still left with a large and robust sample that would still represent the population, allowing us to preserve data integrity.

In addition to incomplete entries, we also standardized key demographic columns like gender and race. Since the survey allowed for free-form text based responses, many respondents answered “man”, “male”, “woman” or “female” among many other responses. We decided to group these into 4 different categories just based on attempting to keep each category as balanced as possible, these categories were Man, Woman, Non-Binary and Other. Similarly, for race, we decided to group responses into broader racial groups such as Hispanic, White, Asian, Black or African American, or Other. This allowed us to retain a level of demographic specificity while ensuring that the category had a consistent set of values.

After ensuring that we dropped incomplete entries and ensuring that our key categories had consistent data, the next step was to address sections with poorly written or dirty data. Since the majority of respondents in our dataset came from the United States, United Kingdom and Canada, we decided to focus our research on these three countries for a more accurate and meaningful comparison. To support this comparison, we had to standardize country names as respondents often entered country names in inconsistent formats such as US, USA or America to indicate the United States. It was important to normalize the country data to consistent values to ensure accurate grouping and filtering later on in analysis. We created a mapping of common variations as well as common misspellings of the United States into “USA”, United Kingdom into “UK” and Canada into “CANADA”. 

To complete the data cleaning process, we cleaned up the formatting of experience levels entries in the data. These entries had inconsistent formatting such as “2 - 4 years”, “2-4years”, and “1 year or less”. We standardized the way experience levels into a numerical form where it was represented as two numbers separated by a hyphen such as “2-4” to represent “2-4 years” of experience. This binning of experience values made it far easier for grouping functions later on in analysis as well as having it in numerical format allowed for further feature engineering stages that may leverage these values. With this final transformation done to the data, we performed a verification of the data, checking null values, confirming data types and doing a final check that our transformations occurred, ensuring that the data was ready for statistical analysis and modeling. 

# Data Analysis

Exploratory data analysis was used to visualize the distribution and impact of features. This included plots for the overall country data distribution, salary distributions within each country, and factors that influenced salary distributions within and across countries. A standard color encoding was used for the visualizations performed for each country: Green, Purple, and Orange for USA, CANADA, and UK respectively. Tools such as matplotlib, seaborn, and pandas were used for our visualizations. 

**Country-Level Distributions:**
Plotted the amount of data available for each country to understand bias, imbalance in data, and to inform our model training process later on. With a majority of data available only in the U.S., Canada, and UK, we proceeded with our analyses on these three countries. Boxplots of annual income were plotted to understand salary distributions in each country.

**Within-Country Comparisons:**
We used boxplots to evaluate how salary is influenced by: Education Level, Years of Experience, Industry, Gender, and Race. 
For U.S. respondents, we further explored state-level salary differences, identifying the top and bottom five states by median salary. The top five and bottom five states based on median salaries were analyzed to examine the differences within the United States. 


# Results

Here, present your results in the form of figures and tables. In the text, explain what you have observed in an organized manner by making frequent references to figures and tables as it applies. You should always first state your observations (Results). Just state what you see. This should be 1-2 pages long but could be longer if you have more figures.

# Discussion

Then explain how you interpret them to reach conclusions (Discussion). In many reports/papers Results and Discussion are in separate sections. You want to be able to separate what is a result and then here weave together the "why" so how do the results and the introduction fit together. Again, you can use enumeration/itemization as needed. The Discussion section should be 1-2 pages long.

# Conclusion

Briefly explain the "take home messages" you have derived from your project and discuss what else can be done to build on your results. The conclusion should contain at most 10 sentences.

# Roles

Provide details of each team member's roles and achievements in the project

# References

Provide a list of the references you used to support your points throughout the report. Do not cite Wikipedia (if you used Wikipedia, find the resource cited there, make sure that it is indeed supporting your point, then cite that resource). Literature is not always to be used because they have statements that support your points, you can also be critical of the literature and/or use it to show that the state-of-the-art is not strong.

Instructor's Additional Notes

1.  Your report should not be any longer than 12 pages, excluding the list of references. It is ok if you need a page or 2 if you have some exceptional novel figures you are adding.

2.  Your report should include at most 5 figures and tables in total. You can combine multiple figures into a single figure by using separate panels, but each figure should be coherent within itself. You must have figure captions and if you have multiple figures then you need to describe each of them and have labels for each.

3.  Figure captions should have a title (in bold) followed by a clear description of how to read the figure. The purpose of the caption is to help the reader understand the figure. Your observations from the figure should not be included in the caption, they should be stated in the text.

4.  Figures should be clearly visible and all material displayed in the figure should be useful to convey an idea and or to make an observation. You should not include unreadable or irrelevant figures just because they look fancy. You should include the maximum/minimum number of Figures to tell your story (no more or less).

5.  You should definitely mention your figure or table in the text. It could be what it conveys or what it shows/results.

6.  Label your code chuncks. You can use R, Python, or both. Your code should be integrated in this report. In the final report, I don't want to see the code in the final output, but you need to turn in the .qmd file so I can see your code. It must be commented well.

7.  Use a .bib file for references. You can change the .qmd file. Check out the Quatro website for ways to improve output. You can change output to html if need be.
